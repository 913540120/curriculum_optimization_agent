"""æ”¶æ•›æ€§æ£€æŸ¥å™¨ - åˆ¤æ–­ä¼˜åŒ–è¿‡ç¨‹æ˜¯å¦æ”¶æ•›"""

from typing import Dict, List, Any

class ConvergenceChecker:
    """æ”¶æ•›æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self, convergence_threshold: float = 0.85):
        """
        åˆå§‹åŒ–æ”¶æ•›æ£€æŸ¥å™¨
        
        Args:
            convergence_threshold: æ”¶æ•›é˜ˆå€¼
        """
        self.convergence_threshold = convergence_threshold
        print(f"ğŸ“Š æ”¶æ•›æ€§æ£€æŸ¥å™¨å·²åˆå§‹åŒ–ï¼Œé˜ˆå€¼: {convergence_threshold}")

    def is_converged(self, optimization_state: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥ä¼˜åŒ–æ˜¯å¦æ”¶æ•›
        
        Args:
            optimization_state: ä¼˜åŒ–çŠ¶æ€
            
        Returns:
            bool: æ˜¯å¦æ”¶æ•›
        """
        print("ğŸ“Š æ­£åœ¨æ£€æŸ¥æ”¶æ•›æ€§...")
        
        # è·å–æ”¶æ•›æŒ‡æ ‡
        metrics = self._calculate_convergence_metrics(optimization_state)
        
        # è®¡ç®—ç»¼åˆæ”¶æ•›åˆ†æ•°
        convergence_score = self._calculate_convergence_score(metrics)
        
        print(f"ğŸ“Š æ”¶æ•›åˆ†æ•°: {convergence_score:.3f}, é˜ˆå€¼: {self.convergence_threshold}")
        
        is_converged = convergence_score >= self.convergence_threshold
        
        if is_converged:
            print("âœ… ä¼˜åŒ–å·²æ”¶æ•›")
        else:
            print("ğŸ”„ ä¼˜åŒ–å°šæœªæ”¶æ•›")
        
        return is_converged

    def _calculate_convergence_metrics(self, state: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—æ”¶æ•›æ€§æŒ‡æ ‡"""
        metrics = {
            "suggestion_reduction_rate": 0.0,
            "conflict_severity_score": 1.0,
            "stakeholder_satisfaction_avg": 0.0,
            "curriculum_stability_score": 0.0,
            "consensus_degree": 0.0
        }
        
        # å»ºè®®å‡å°‘ç‡
        current_round = state.get("optimization_round", 1)
        if current_round > 1:
            # æ¯”è¾ƒå½“å‰è½®æ¬¡å’Œå‰ä¸€è½®æ¬¡çš„å»ºè®®æ•°é‡
            current_suggestions = len(state.get("stakeholder_feedback", {}))
            if current_suggestions == 0:
                metrics["suggestion_reduction_rate"] = 1.0
            else:
                metrics["suggestion_reduction_rate"] = max(0, 1 - current_suggestions / 10)  # å‡è®¾åˆå§‹æœ‰10ä¸ªå»ºè®®
        
        # å†²çªä¸¥é‡åº¦
        conflicts = state.get("conflict_analysis", [])
        if conflicts:
            avg_severity = sum(c.get("severity_score", 0.5) for c in conflicts) / len(conflicts)
            metrics["conflict_severity_score"] = 1.0 - avg_severity  # å†²çªè¶Šå°‘åˆ†æ•°è¶Šé«˜
        else:
            metrics["conflict_severity_score"] = 1.0
        
        # åˆ©ç›Šç›¸å…³è€…æ»¡æ„åº¦
        satisfaction = state.get("stakeholder_satisfaction", {})
        if satisfaction:
            metrics["stakeholder_satisfaction_avg"] = sum(satisfaction.values()) / len(satisfaction)
        else:
            metrics["stakeholder_satisfaction_avg"] = 0.8  # é»˜è®¤æ»¡æ„åº¦
        
        # åŸ¹å…»æ–¹æ¡ˆç¨³å®šæ€§ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        versions = state.get("curriculum_versions", [])
        if len(versions) > 1:
            # å¦‚æœç‰ˆæœ¬å˜åŒ–è¾ƒå°‘ï¼Œè¯´æ˜è¶‹äºç¨³å®š
            metrics["curriculum_stability_score"] = min(1.0, 0.5 + 0.1 * len(versions))
        else:
            metrics["curriculum_stability_score"] = 0.7
        
        # å…±è¯†ç¨‹åº¦
        consensus_points = state.get("consensus_points", [])
        metrics["consensus_degree"] = min(1.0, len(consensus_points) / 5)  # å‡è®¾5ä¸ªå…±è¯†ç‚¹ä¸ºæ»¡åˆ†
        
        return metrics

    def _calculate_convergence_score(self, metrics: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆæ”¶æ•›åˆ†æ•°"""
        
        # å„æŒ‡æ ‡æƒé‡
        weights = {
            "suggestion_reduction_rate": 0.2,
            "conflict_severity_score": 0.3,
            "stakeholder_satisfaction_avg": 0.2,
            "curriculum_stability_score": 0.2,
            "consensus_degree": 0.1
        }
        
        # åŠ æƒå¹³å‡
        score = sum(metrics[key] * weights[key] for key in weights if key in metrics)
        
        return min(score, 1.0)

    def get_convergence_report(self, optimization_state: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ”¶æ•›æ€§æŠ¥å‘Š"""
        
        metrics = self._calculate_convergence_metrics(optimization_state)
        convergence_score = self._calculate_convergence_score(metrics)
        is_converged = convergence_score >= self.convergence_threshold
        
        report = {
            "is_converged": is_converged,
            "convergence_score": convergence_score,
            "threshold": self.convergence_threshold,
            "metrics": metrics,
            "recommendations": []
        }
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        if not is_converged:
            if metrics["conflict_severity_score"] < 0.7:
                report["recommendations"].append("éœ€è¦è¿›ä¸€æ­¥è§£å†³åˆ©ç›Šç›¸å…³è€…ä¹‹é—´çš„å†²çª")
            
            if metrics["stakeholder_satisfaction_avg"] < 0.6:
                report["recommendations"].append("éœ€è¦æé«˜åˆ©ç›Šç›¸å…³è€…çš„æ»¡æ„åº¦")
            
            if metrics["consensus_degree"] < 0.5:
                report["recommendations"].append("éœ€è¦å½¢æˆæ›´å¤šå…±è¯†ç‚¹")
        
        return report